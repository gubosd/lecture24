{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리 딥러닝\n",
    "# 단어 임베딩\n",
    "- word2vec 알고리즘\n",
    "- 한 단어는 단어집의 갯수만큼의 차원을 가진다. (원핫인코딩 형태)\n",
    "- 이렇게 큰 단어의 차원을 줄여주는 것이 단어 임베딩이다.\n",
    "- 단어 임베딩은 학습을 통해 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='embedding.jpg' />\n",
    "(출처: http://th-mayer.de/pycon2018/#/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDb 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_data in module tensorflow.python.keras.datasets.imdb:\n",
      "\n",
      "load_data(path='imdb.npz', num_words=None, skip_top=0, maxlen=None, seed=113, start_char=1, oov_char=2, index_from=3, **kwargs)\n",
      "    Loads the [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
      "    \n",
      "    This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment\n",
      "    (positive/negative). Reviews have been preprocessed, and each review is\n",
      "    encoded as a list of word indexes (integers).\n",
      "    For convenience, words are indexed by overall frequency in the dataset,\n",
      "    so that for instance the integer \"3\" encodes the 3rd most frequent word in\n",
      "    the data. This allows for quick filtering operations such as:\n",
      "    \"only consider the top 10,000 most\n",
      "    common words, but eliminate the top 20 most common words\".\n",
      "    \n",
      "    As a convention, \"0\" does not stand for a specific word, but instead is used\n",
      "    to encode any unknown word.\n",
      "    \n",
      "    Arguments:\n",
      "        path: where to cache the data (relative to `~/.keras/dataset`).\n",
      "        num_words: integer or None. Words are\n",
      "            ranked by how often they occur (in the training set) and only\n",
      "            the `num_words` most frequent words are kept. Any less frequent word\n",
      "            will appear as `oov_char` value in the sequence data. If None,\n",
      "            all words are kept. Defaults to None, so all words are kept.\n",
      "        skip_top: skip the top N most frequently occurring words\n",
      "            (which may not be informative). These words will appear as\n",
      "            `oov_char` value in the dataset. Defaults to 0, so no words are\n",
      "            skipped.\n",
      "        maxlen: int or None. Maximum sequence length.\n",
      "            Any longer sequence will be truncated. Defaults to None, which\n",
      "            means no truncation.\n",
      "        seed: int. Seed for reproducible data shuffling.\n",
      "        start_char: int. The start of a sequence will be marked with this\n",
      "            character. Defaults to 1 because 0 is usually the padding character.\n",
      "        oov_char: int. The out-of-vocabulary character.\n",
      "            Words that were cut out because of the `num_words` or\n",
      "            `skip_top` limits will be replaced with this character.\n",
      "        index_from: int. Index actual words with this index and higher.\n",
      "        **kwargs: Used for backwards compatibility.\n",
      "    \n",
      "    Returns:\n",
      "        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n",
      "    \n",
      "        **x_train, x_test**: lists of sequences, which are lists of indexes\n",
      "          (integers). If the num_words argument was specific, the maximum\n",
      "          possible index value is `num_words - 1`. If the `maxlen` argument was\n",
      "          specified, the largest possible sequence length is `maxlen`.\n",
      "    \n",
      "        **y_train, y_test**: lists of integer labels (1 or 0).\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: in case `maxlen` is so low\n",
      "            that no input sequence could be kept.\n",
      "    \n",
      "    Note that the 'out of vocabulary' character is only used for\n",
      "    words that were present in the training set but are not included\n",
      "    because they're not making the `num_words` cut here.\n",
      "    Words that were not seen in the training set but are in the test set\n",
      "    have simply been skipped.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(keras.datasets.imdb.load_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "d:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "d:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=10000)\n",
    "    # 가장 빈도가 높은 만개의 단어만 추출 (0~9999)\n",
    "    # 0 : padding 용도\n",
    "    # 1 : start character\n",
    "    # 2 : out-of-vocabulary (10000번 이상의 단어를 표시)\n",
    "    # 3 : 사용되지 않았음\n",
    "    # 빈도가 높은 순서 대로 4번 부터 번호를 붙인다\n",
    "    # C:\\Users\\사용자아이디\\.keras\\datasets\\imdb.npz 에 저장됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,), (25000,), (25000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 14 22 16 43 530 973 1622 1385 65 458 4468 66 3941 4 173 36 256 5 25 100 43 838 112 50 670 2 9 35 480 284 5 150 4 172 112 167 2 336 385 39 4 172 4536 1111 17 546 38 13 447 4 192 50 16 6 147 2025 19 14 22 4 1920 4613 469 4 22 71 87 12 16 43 530 38 76 15 13 1247 4 22 17 515 17 12 16 626 18 2 5 62 386 12 8 316 8 106 5 4 2223 5244 16 480 66 3785 33 4 130 12 16 38 619 5 25 124 51 36 135 48 25 1415 33 6 22 12 215 28 77 52 5 14 407 16 82 2 8 4 107 117 5952 15 256 4 2 7 3766 5 723 36 71 43 530 476 26 400 317 46 7 4 2 1029 13 104 88 4 381 15 297 98 32 2071 56 26 141 6 194 7486 18 4 226 22 21 134 476 26 480 5 144 30 5535 18 51 36 28 224 92 25 104 4 226 65 16 38 1334 88 12 16 283 5 16 4472 113 103 32 15 16 5345 19 178 32\n"
     ]
    }
   ],
   "source": [
    "print(*X_train[0])\n",
    "    # 단어들의 인덱스임 (1~9999 인 단어만 있음)\n",
    "    # 첫번째 1은 start character\n",
    "    # 2 는 10000개의 단어 중에 없는 단어임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9999, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(l) for l in X_train]), min([min(l) for l in X_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2494, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list = [len(l) for l in X_train]\n",
    "max(len_list), min(len_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18309341e50>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sorted(len_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(len_list, bins=100)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12500, 12500], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어사전 얻기(단어, 인덱스)\n",
    "- C:\\Users\\사용자아이디\\\\.keras\\datasets\\imdb_word_index.json\n",
    "- X_train 의 인덱스 값에서 3을 빼줘야 word_index 의 인덱스 값에 해당함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = keras.datasets.imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88584, dict)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2i), type(w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn 34701\n",
      "tsukino 52006\n",
      "nunnery 52007\n",
      "sonja 16816\n",
      "vani 63951\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for k in w2i:\n",
    "    print(k, w2i[k])\n",
    "    n += 1\n",
    "    if n>=5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i2w = {}\n",
    "\n",
    "for k,v in w2i.items():\n",
    "    i2w[v] = k\n",
    "    \n",
    "# i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', \"'l'\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w[1], i2w[88584] # i2w[0] 은 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 문장의 단어를 20개로 제한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  1,  2,  3],\n",
       "       [ 0,  0,  4,  5,  6],\n",
       "       [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.sequence.pad_sequences([[1,2,3],[4,5,6],[1,2,3,4,5,6,7,8,9,10]], maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=100)\n",
    "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
      "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
      "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
      "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
      " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
      "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
      "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
      "  178   32]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train[0]) # 뒷부분만 남긴다 (100단어 보다 작은 경우 앞에 0을 채운다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    1,  778,  128,   74,   12,  630,  163,   15,    4,\n",
       "       1766, 7982, 1051,    2,   32,   85,  156,   45,   40,  148,  139,\n",
       "        121,  664,  665,   10,   10, 1361,  173,    4,  749,    2,   16,\n",
       "       3804,    8,    4,  226,   65,   12,   43,  127,   24,    2,   10,\n",
       "         10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X begins better than it ends funny that the russian submarine crew X all other actors it's like those scenes where documentary shots br br spoiler part the message X was contrary to the whole story it just does not X br br "
     ]
    }
   ],
   "source": [
    "for i in X_train[5]:\n",
    "    if i>3: print(i2w[i-3], end=' ')\n",
    "    else: print('X', end=' ')\n",
    "    \n",
    "# X_train 에서는 4번 부터 단어 번호가 붙고, 단어사전에는 1번 부터 번호를 붙임\n",
    "# 그래서 3을 빼 주어야 해당 단어가 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정리\n",
    "- X_train 은 총 25,000개의 문장(Sequence) 이다.\n",
    "- 한 문장(Sequence)는 100개의 단어로 이루어져 있다.\n",
    "- 각 단어에 해당하는 번호는 0~9999 로 총 1만개로 이루어져 있다.\n",
    "- 단어 번호(인덱스)가 정수 하나이지만 실제로는 1만개의 성분인 원핫인코딩 벡터로 생각해야 한다.\n",
    "- (25000, 100, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras 의 Embedding 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Embedding in module tensorflow.python.keras.layers.embeddings:\n",
      "\n",
      "class Embedding(tensorflow.python.keras.engine.base_layer.Layer)\n",
      " |  Embedding(*args, **kwargs)\n",
      " |  \n",
      " |  Turns positive integers (indexes) into dense vectors of fixed size.\n",
      " |  \n",
      " |  e.g. `[[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]`\n",
      " |  \n",
      " |  This layer can only be used as the first layer in a model.\n",
      " |  \n",
      " |  Example:\n",
      " |  \n",
      " |  >>> model = tf.keras.Sequential()\n",
      " |  >>> model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
      " |  >>> # The model will take as input an integer matrix of size (batch,\n",
      " |  >>> # input_length), and the largest integer (i.e. word index) in the input\n",
      " |  >>> # should be no larger than 999 (vocabulary size).\n",
      " |  >>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
      " |  >>> # dimension.\n",
      " |  >>> input_array = np.random.randint(1000, size=(32, 10))\n",
      " |  >>> model.compile('rmsprop', 'mse')\n",
      " |  >>> output_array = model.predict(input_array)\n",
      " |  >>> print(output_array.shape)\n",
      " |  (32, 10, 64)\n",
      " |  \n",
      " |  Args:\n",
      " |    input_dim: Integer. Size of the vocabulary,\n",
      " |      i.e. maximum integer index + 1.\n",
      " |    output_dim: Integer. Dimension of the dense embedding.\n",
      " |    embeddings_initializer: Initializer for the `embeddings`\n",
      " |      matrix (see `keras.initializers`).\n",
      " |    embeddings_regularizer: Regularizer function applied to\n",
      " |      the `embeddings` matrix (see `keras.regularizers`).\n",
      " |    embeddings_constraint: Constraint function applied to\n",
      " |      the `embeddings` matrix (see `keras.constraints`).\n",
      " |    mask_zero: Boolean, whether or not the input value 0 is a special \"padding\"\n",
      " |      value that should be masked out.\n",
      " |      This is useful when using recurrent layers\n",
      " |      which may take variable length input.\n",
      " |      If this is `True`, then all subsequent layers\n",
      " |      in the model need to support masking or an exception will be raised.\n",
      " |      If mask_zero is set to True, as a consequence, index 0 cannot be\n",
      " |      used in the vocabulary (input_dim should equal size of\n",
      " |      vocabulary + 1).\n",
      " |    input_length: Length of input sequences, when it is constant.\n",
      " |      This argument is required if you are going to connect\n",
      " |      `Flatten` then `Dense` layers upstream\n",
      " |      (without it, the shape of the dense outputs cannot be computed).\n",
      " |  \n",
      " |  Input shape:\n",
      " |    2D tensor with shape: `(batch_size, input_length)`.\n",
      " |  \n",
      " |  Output shape:\n",
      " |    3D tensor with shape: `(batch_size, input_length, output_dim)`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Embedding\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None, **kwargs)\n",
      " |  \n",
      " |  build = wrapper(instance, input_shape)\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          *args: Additional positional arguments. Currently unused.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  compute_output_shape = wrapper(instance, input_shape)\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Note that `get_config()` does not guarantee to return a fresh copy of dict\n",
      " |      every time it is called. The callers should make a copy of the returned dict\n",
      " |      if they want to modify it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Args:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |        - If the layer is not built, the method will call `build`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = tf.keras.metrics.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(inputs))\n",
      " |          self.add_metric(tf.reduce_sum(inputs), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Args:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The variable created.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_output_signature(self, input_signature)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first input node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first output node of the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Args:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use!\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer, as NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of NumPy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of NumPy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from NumPy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function, by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a `Dense` layer returns a list of two values: the kernel matrix\n",
      " |      and the bias vector. These can be used to set the weights of another\n",
      " |      `Dense` layer:\n",
      " |      \n",
      " |      >>> layer_a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = layer_a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> layer_a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = layer_b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> layer_b.set_weights(layer_a.get_weights())\n",
      " |      >>> layer_b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: a list of NumPy arrays. The number\n",
      " |          of arrays and their shape must match\n",
      " |          number of the dimensions of the weights\n",
      " |          of the layer (i.e. it should match the\n",
      " |          output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the provided weights list does not match the\n",
      " |          layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  compute_dtype\n",
      " |      The dtype of the layer's computations.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.dtype`, the dtype of\n",
      " |      the weights.\n",
      " |      \n",
      " |      Layers automatically cast their inputs to the compute dtype, which causes\n",
      " |      computations and the output to be in the compute dtype as well. This is done\n",
      " |      by the base Layer class in `Layer.__call__`, so you do not have to insert\n",
      " |      these casts if implementing your own layer.\n",
      " |      \n",
      " |      Layers often perform certain internal computations in higher precision when\n",
      " |      `compute_dtype` is float16 or bfloat16 for numeric stability. The output\n",
      " |      will still typically be float16 or bfloat16 in such cases.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The layer's compute dtype.\n",
      " |  \n",
      " |  dtype\n",
      " |      The dtype of the layer weights.\n",
      " |      \n",
      " |      This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless\n",
      " |      mixed precision is used, this is the same as `Layer.compute_dtype`, the\n",
      " |      dtype of the layer's computations.\n",
      " |  \n",
      " |  dtype_policy\n",
      " |      The dtype policy associated with this layer.\n",
      " |      \n",
      " |      This is an instance of a `tf.keras.mixed_precision.Policy`.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> len(model.losses)\n",
      " |      0\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> len(model.losses)\n",
      " |      1\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of `Metric` objects.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |      Sequence of non-trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |  \n",
      " |  variable_dtype\n",
      " |      Alias of `Layer.dtype`, the dtype of the weights.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Note: This will not track the weights of nested `tf.Modules` that are not\n",
      " |      themselves Keras layers.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x1830950feb0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = Embedding(10, 2, input_length=5)\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 5, 2), dtype=float32, numpy=\n",
       "array([[[-0.02053005,  0.04023727],\n",
       "        [-0.00324385,  0.02492559],\n",
       "        [ 0.00584049,  0.03094316],\n",
       "        [ 0.03028831, -0.01241819],\n",
       "        [-0.01538276,  0.01883492]]], dtype=float32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins = np.arange(5).reshape(1,5)\n",
    "outs = emb(ins)\n",
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_1/embeddings:0' shape=(10, 2) dtype=float32, numpy=\n",
       " array([[-0.02053005,  0.04023727],\n",
       "        [-0.00324385,  0.02492559],\n",
       "        [ 0.00584049,  0.03094316],\n",
       "        [ 0.03028831, -0.01241819],\n",
       "        [-0.01538276,  0.01883492],\n",
       "        [-0.04580216, -0.00103288],\n",
       "        [ 0.00714261,  0.01175598],\n",
       "        [ 0.0465905 , -0.03376883],\n",
       "        [ 0.00590042, -0.04715072],\n",
       "        [-0.01531485, -0.02500334]], dtype=float32)>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=100))\n",
    "    # 샘플당 단어 100개가 들어옴. 단어의 인덱스는 0~9999\n",
    "    # 단어의 차원이 10000 에서 8로 줄어듬\n",
    "    # 출력 차원은 (None, 100, 8) => 희소벡터를 밀집벡터로 변환\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embedding 은 단어인덱스의 열(Sequence)를 입력받아 각 단어들을 줄어든 차원의 벡터로 출력한다 (word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 8)            80000     \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 32)                25632     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 105,665\n",
      "Trainable params: 105,665\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "79/79 [==============================] - 1s 11ms/step - loss: 0.6775 - acc: 0.6108 - val_loss: 0.6204 - val_acc: 0.7336\n",
      "Epoch 2/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.4972 - acc: 0.8090 - val_loss: 0.4137 - val_acc: 0.8246\n",
      "Epoch 3/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.3296 - acc: 0.8714 - val_loss: 0.3512 - val_acc: 0.8460\n",
      "Epoch 4/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.2574 - acc: 0.9024 - val_loss: 0.3322 - val_acc: 0.8530\n",
      "Epoch 5/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.2109 - acc: 0.9238 - val_loss: 0.3327 - val_acc: 0.8550\n",
      "Epoch 6/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.1736 - acc: 0.9416 - val_loss: 0.3406 - val_acc: 0.8516\n",
      "Epoch 7/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.1427 - acc: 0.9569 - val_loss: 0.3581 - val_acc: 0.8456\n",
      "Epoch 8/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.1157 - acc: 0.9686 - val_loss: 0.3763 - val_acc: 0.8466\n",
      "Epoch 9/10\n",
      "79/79 [==============================] - 1s 9ms/step - loss: 0.0913 - acc: 0.9797 - val_loss: 0.3990 - val_acc: 0.8428\n",
      "Epoch 10/10\n",
      "79/79 [==============================] - 1s 8ms/step - loss: 0.0701 - acc: 0.9864 - val_loss: 0.4226 - val_acc: 0.8430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1830d97f790>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(X_train, y_train, batch_size=256, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1830b991ee0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(model.history.history['acc'])\n",
    "plt.plot(model.history.history['val_acc'])\n",
    "plt.legend(['acc', 'val_acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 8)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws[0].shape # 단어 만개에 대한 가중치 (또는 특성 벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1830b4343a0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(ws[0][:30])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 비슷한 패턴의 행들에 해당하는 단어들은 문장에서 같이 등장할 가능성이 크다.\n",
    "- 위에서 14번과 16번 단어는 비슷한 패턴이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('this', 'was', 'movie')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w[14-3], i2w[16-3], i2w[20-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i2w[21-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3121, 2384,  862, 4063,  753, 3179, 3316, 1149,  437,  249],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = ws[0]\n",
    "n = w2i['excellent'] + 3\n",
    "\n",
    "distance = []\n",
    "\n",
    "for i in range(10000):\n",
    "    distance.append(((mat[n]-mat[i])**2).sum())\n",
    "    \n",
    "result = np.argsort(distance)[-10:]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wasting\n",
      "insult\n",
      "poorly\n",
      "unwatchable\n",
      "dull\n",
      "mst3k\n",
      "incoherent\n",
      "pointless\n",
      "waste\n",
      "worst\n"
     ]
    }
   ],
   "source": [
    "for i in result:\n",
    "    print(i2w[i-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe 적용\n",
    "- glove 는 스탠포드 대학교에서 만든 2014년 영문 위키피디아를 사용하여 사전에 계산한 임베딩임\n",
    "- 구글의 Word2vec 와 함께 인기가 많다\n",
    "- https://nlp.stanford.edu/projects/glove\n",
    "- https://bit.ly/2NIJwdb 에서 glove.6B.zip 다운로드 (이 중 glove.6B.100d.txt 사용)\n",
    "- 단어 40만개, 100차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('F:\\\\datasets\\\\glove.6B.100d.txt', encoding='utf8')\n",
    "s = f.readline()\n",
    "s2 = f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the -0.038194 -0.24487 0.72812 -0.39961 0.083172 0.043953 -0.39141 0.3344 -0.57545 0.087459 0.28787 -0.06731 0.30906 -0.26384 -0.13231 -0.20757 0.33395 -0.33848 -0.31743 -0.48336 0.1464 -0.37304 0.34577 0.052041 0.44946 -0.46971 0.02628 -0.54155 -0.15518 -0.14107 -0.039722 0.28277 0.14393 0.23464 -0.31021 0.086173 0.20397 0.52624 0.17164 -0.082378 -0.71787 -0.41531 0.20335 -0.12763 0.41367 0.55187 0.57908 -0.33477 -0.36559 -0.54857 -0.062892 0.26584 0.30205 0.99775 -0.80481 -3.0243 0.01254 -0.36942 2.2167 0.72201 -0.24978 0.92136 0.034514 0.46745 1.1079 -0.19358 -0.074575 0.23353 -0.052062 -0.22044 0.057162 -0.15806 -0.30798 -0.41625 0.37972 0.15006 -0.53212 -0.2055 -1.2526 0.071624 0.70565 0.49744 -0.42063 0.26148 -1.538 -0.30223 -0.073438 -0.28312 0.37104 -0.25217 0.016215 -0.017099 -0.38984 0.87424 -0.72569 -0.51058 -0.52028 -0.1459 0.8278 0.27062\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', -0.10767 0.11053 0.59812 -0.54361 0.67396 0.10663 0.038867 0.35481 0.06351 -0.094189 0.15786 -0.81665 0.14172 0.21939 0.58505 -0.52158 0.22783 -0.16642 -0.68228 0.3587 0.42568 0.19021 0.91963 0.57555 0.46185 0.42363 -0.095399 -0.42749 -0.16567 -0.056842 -0.29595 0.26037 -0.26606 -0.070404 -0.27662 0.15821 0.69825 0.43081 0.27952 -0.45437 -0.33801 -0.58184 0.22364 -0.5778 -0.26862 -0.20425 0.56394 -0.58524 -0.14365 -0.64218 0.0054697 -0.35248 0.16162 1.1796 -0.47674 -2.7553 -0.1321 -0.047729 1.0655 1.1034 -0.2208 0.18669 0.13177 0.15117 0.7131 -0.35215 0.91348 0.61783 0.70992 0.23955 -0.14571 -0.37859 -0.045959 -0.47368 0.2385 0.20536 -0.18996 0.32507 -1.1112 -0.36341 0.98679 -0.084776 -0.54008 0.11726 -1.0194 -0.24424 0.12771 0.013884 0.080374 -0.35414 0.34951 -0.7226 0.37549 0.4441 -0.99059 0.61214 -0.35111 -0.83155 0.45293 0.082577\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = []\n",
    "vec = []\n",
    "\n",
    "f = open('f:/datasets/glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    l = line.strip().split()\n",
    "    word.append(l[0])\n",
    "    vec.append([float(n) for n in l[1:]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('the', 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word[0], len(vec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0\n",
    "\n",
    "f = open('f:/datasets/glove.6B.100d.txt', encoding='utf8')\n",
    "for line in f:\n",
    "    n += 1\n",
    "f.close()\n",
    "\n",
    "n # 40만 개의 단어"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
